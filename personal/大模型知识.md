### **模型架构基础**

1. **Transformer核心机制**
   * **自注意力**（Self-Attention）：QKV矩阵计算、多头注意力（Multi-Head）
   * **位置编码**：正弦函数（原始Transformer） vs 可学习嵌入（如BERT）
   * **层归一化**（LayerNorm）与残差连接（Residual Connection）
2. **主流架构变体**
   * **编码器-解码器**（T5、BART）
   * **纯解码器**（GPT系列、LLaMA）：自回归生成
   * **纯编码器**（BERT、RoBERTa）：双向上下文理解
3. **扩展架构**
   * **稀疏化**：Mixture of Experts（MoE，如Switch Transformer）
   * **长上下文优化**：ALiBi位置编码、FlashAttention加速

**Transformer的自注意力机制**

它通过计算输入序列中所有位置之间的相关性，动态地为每个位置分配不同的权重，从而捕捉序列内部的依赖关系（无论距离远近）。

核心思想：

自注意力机制的目标是让序列中的每个元素（如单词或像素）通过“关注”序列中的其他元素，生成一个更具上下文信息的表示。其优势在于：

* **并行计算**：不同于RNN的逐步处理，自注意力可以同时计算所有位置的关系。
* **长程依赖**：直接建模任意距离元素之间的关系，避免梯度消失问题。

输入表示：

![](https://apijoyspace.jd.com/v1/files/71G4Q5xy7Bu5EWrp68QF/link)

计算注意力权重：

![](https://apijoyspace.jd.com/v1/files/2zVjq9KLrTdFYKpGgbfH/link)

加权求和：

![](https://apijoyspace.jd.com/v1/files/RQEzrJetBG8kWemRhgsj/link)

多头注意力：

![](https://apijoyspace.jd.com/v1/files/O3lw65M1j5QGpxY11uCU/link)

有效性分析：

* **动态权重分配**：根据输入内容自适应调整关注的重点（如代词可能关注其指代的名词）。
* **位置无关性**：通过位置编码（Positional Encoding）注入序列顺序信息，弥补自注意力本身的无序性。
* **可解释性**：注意力权重可视化为不同位置的重要性（如机器翻译中对齐关系）。

**信息在embedding为向量以后在transformer中会经历什么**

![](https://apijoyspace.jd.com/v1/files/Iecjl2fCC4hrId8mkIZW/link)

![](https://apijoyspace.jd.com/v1/files/837q42hSzw9X8WjiL7gn/link)

![](https://apijoyspace.jd.com/v1/files/czwC5blTa4y8x7TBAYtc/link)

![](https://apijoyspace.jd.com/v1/files/wvuz8Km18WjlSJGMYj6X/link)

### **训练与优化**

1. **预训练技术**
   * **目标函数**：
     * 自回归语言建模（GPT）
     * 掩码语言建模（BERT）
     * 置换语言建模（XLNet）
   * **数据并行策略**：DP（数据并行）、TP（张量并行）、PP（流水线并行）
2. **微调方法**
   * **全参数微调**：计算成本高，适合领域适配
   * **参数高效微调**（PEFT）：
     * LoRA（低秩适配）
     * Adapter（插入小型网络）
     * Prefix-Tuning（学习提示前缀）
3. **对齐技术**
   * **RLHF**（基于人类反馈的强化学习）：SFT → 奖励建模 → PPO优化
   * **DPO**（直接偏好优化）：替代RLHF的无奖励模型方法

### **推理与部署**

1. **推理加速**
   * **KV Cache**：缓存历史键值对，减少重复计算
   * **量化**：FP16 → INT8/INT4（如GPTQ、AWQ算法）
   * **推测解码**（Speculative Decoding）：小模型草稿 + 大模型验证
2. **服务化架构**
   * **批处理**（Batching）与动态批处理
   * **流式响应**（Server-Sent Events）
   * **负载均衡**：多GPU实例 + 自动扩缩容

### **应用开发技术**

1. **提示工程（Prompt Engineering）**
   * **技术**：Few-shot、Chain-of-Thought（CoT）、Self-Consistency
   * **模板设计**：系统提示（System Prompt）约束模型行为
   * **参数调控**：Temperature（创造性）、Top-p（多样性）
2. **检索增强生成（RAG）**
   * **检索器**：稠密检索（如BGE嵌入） + 稀疏检索（BM25）
   * **向量数据库**：FAISS、Milvus、Pinecone
   * **优化点**：查询重写、HyDE（假设性文档嵌入）
3. **工具调用（Tool Use）**
   * **流程**：模型生成JSON请求 → 执行工具 → 返回结果
   * **框架**：OpenAI Function Calling、LangChain Tools

### **评估与迭代**

1. **评估指标**
   * **生成质量**：BLEU、ROUGE、BERTScore
   * **事实性**：检索命中率、引用准确性
   * **安全性**：有害内容检出率
2. **数据飞轮（Data Flywheel）**
   * 收集用户反馈 → 筛选高质量数据 → 迭代训练

### **前沿扩展**

1. **多模态模型**
   * **架构**：CLIP（图文对齐）、Flamingo（多模态对话）
   * **应用**：视觉问答（VQA）、跨模态检索
2. **自主智能体（Agent）**
   * **框架**：ReAct（推理+行动）、AutoGPT（自动目标分解）
   * **记忆机制**：向量数据库 + 摘要缓存
3. **小型化技术**
   * **蒸馏**：TinyLLM（从大模型蒸馏知识）
   * **剪枝**：移除冗余注意力头/神经元

### **工具与生态**

### 

1. **开发框架**
   * HuggingFace Transformers、vLLM（高效推理）、LangChain（应用编排）
2. **硬件适配**
   * CUDA优化（如FlashAttention-2）、TPU训练
